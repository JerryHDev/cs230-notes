# Practical Aspects of Deep Learning
23 April 2018
24 April 2018

# Train / Dev / Test Sets
ML is very iterative and you need to spend a lot of time tuning hyperparameters
You need to train #layers, #hidden units, learning rates, activation functions

Traditionally you select a small portion of the data to be training set, holdout cross validation set / dev set, test set
Train models on training set, see which performs best on dev set, evaluate model on test set to get unbiased estimate of algorithm
Most of the data ends up in the training set
60% / 20% / 20% was applicable to datasets of 1000s
With larger and larger datasets, dev and test sets have become much smaller than normal
Dev and test set just need to be representative samples
98 / 1 / 1 split can work on big data sets

Ex: find pictures of cats
Training set: cat pictures from webpages
Dev / test sets: cat pictures from users
Bad split

Make sure the dev and test sets come from the same distribution as the training set
It is ok to not have test set because dev set serves a similar purpose
The dev set is referred to as a test set

Hold out cross validation set

# Bias / Variance
Bias-variance tradeoff

High bias is underfitting
High variance is overfitting

You can plot the decision boundary in 2D but not in higher dimensions
ex:
Train set error: 1%
Dev set error: 11%
You do well on training set, poorly on dev set. You overfit to the training set, high variance - overfitting

ex:
Train set error: 15%
Dev set error: 16%
You do poorly on both training and dev set, high bias - underfitting

ex:
Train set error: 15%
Dev set error: 30%
High bias (bad fit) and high variance (overfit)

Based on assumption that Bayesian erorr is 0%
If the Bayesian error was higher than you would be ok with training and dev errors at that level
Bayesian erorr is 'optimal' error

# Basic Recipe for Machine Learning
1. Ask if the algorithm has high bias? Underfitting
Training data performance --> bigger network, train longer, new architecture
Optimize the bias low
2. Ask if the algorithm has high variance? Overfitting
Dev set performance --> more data, regularization

Bias variance tradeoff
Bias and variance usually go inversely hand in hand

Training a bigger network almost never hurts, doesn't overfit

# Regularization
Ways to prevent overfitting (reducing variance)

## Regularizing for logistic regression
min over[w,b] J(w,b)
J(w, b) = 1 / m * SUM(L(y_hat^(i), y^(i)))

L2 regularization is
 λ / 2m * L2NORM(w)^2 
L2NORM(w) = w^T * W

The regularization term is λ / 2m * L2NORM(W)^2
We normally don't need to regularize b because w has almost all of the parameters, especially in many dimensions

L1 regularization is 
λ / m * L1NORM(W)
L1NORM(w) = SUM(|w|)

If you use L1 regularization makes the model sparse, with a lot of zeroes

λ is the regularization parameters which is set using the development set (holdout cross-validation)

## Regularizing for a neural net
To regularize you add λ / 2m * SUM(||W^[l]||^2)

||w^[l]||^2 is the sum of the squares of all of the elements of w
This is called the frobenius norm of the matrix, denoted with a subscript F

Subscript is the type of the norm

To do gradient descent with this:
dW^[l] = value from backprop + λ / m * W^[l]

L2 regularization is sometimes called weight decay

This makes sense because when you take the derivative of the regularization term with respect to the weights of layer l you get λ / m * W^[l]
The matrix W^[l] always gets a little bit smaller because you always subtract out an extra αλ / m * W[l]
This is called weight decay because you are decaying the values

The weight of the matrix gets smaller over time

**General Idea:** Penalizing weights in all cases such that weights that don't much matter much end up dropped
**Question** How come the decaying weights don't cause the neural network to never converge?

# Why Regulariztion Reduces Overfitting?
Bias is underfitting
Variance is overfitting

ex. We have neural network that is currently overfitting
If you make the regularization to be very high, then it will set the weights very close to 0
A lot of the hidden units will begin to not have any impact
A much simplified neural network emerged, more simplified

It still uses all of the hidden units, each of them just has a smaller effect; you end up with a simpler network

ex. Assume we are using tanh activation function
Smaller values of Z appear almost entirely linear
Only with larger values of Z does the activation function become nonlinear
Is you have a large λ then you get a small set of weights which gives you small Z values which gives you roughly linear tanh(Z) values
This makes every layer roughly linear
This makes the entire neural network roughly linear

Yet this isn't bad because the most important terms stay nonlinear

When implementing regularization we modified the cost function by adding λ / 2m * frobenius(W)^2
Plot the cost function against #iterations
Ensure that cost function decreases monotonically

# Dropout Regularization
Dropout regularization is where you go through each layer and you set a probability of dropping a certain node
You drop a node by removing it?

For each training example you train it with a reduced network?

## Inverted Dropout
ex: We want to apply dropout to layer 3

You dropout values by setting their weight to 0 

keep_prob is the probability that a given hiden unit is kept through the next layer
Example keep_prob would be 0.8
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
a3 = np.multiply(a3, d3)

a3 has dimensions l^[3] by m

ex: a3 is 50 x m dimensional
on average 10 units are dropped out
Z^[4] = w^[4] * a^[3] + b^[4]
20% of the elements of a^[3] will be zeroed out

You need to divide this value by 0.8 so you bump the values back up
a^[3] /= keep_prob
Z^[4] = W^[4] * a^[3] + b^[4]
Inverted dropout technique makes testing easier

Each pass and each training example will have different units zeroed out

## Predictions at Test Time
You are given a^[0], your test data
You don't use dropout during test time
You don't have randomness during test time
This is because when you make predictions during test time you don't want randomness in your predictions

We invert during train test so that we don't need a scaling factor

# Understanding Dropout
On each iteration you are working with a smaller neural network
ex:
We have a unit with 4 inputs
With dropout the inputs can get randomly eliminated
Each node can no longer rely on any one input feature (or node)

The unit is more motivated to spread out its weights
Spreading out the weights has an effect of shrinking the squared norm of the weights

Dropout and L2 regularization have similar effects

Each layer could have a different keep_prob
Larger weight matrices should match up to lower keep_prob
Very small layers can have keep_prob values of 1.0 (keep everything)
The layers with a lot of parameters are the ones with potential issues with overfitting
You can apply dropout to the input features (although this is not done much in practice)
People often use 1.0 or 0.95 for the input features

CV uses a lot of dropout because of the scale of the data
With dropout J is no longer well-defined, a bunch of nodes are randomly cut off
It is harder to check that gradient descent is working 
J now has some randomness

Use keep_prob = 1, check that J decreases monotonically
Then set keep_prob = real value and you will have less variance

# Other Regularization Methods

## Data Augmentation
If you have images, you can flip them horizontally and add it back to the dataset
You can also crop the image
You can also impose random distortions

Early stopping: You run gradient descent and plot training error or 0/1 classification error or cost function against iterations
These should decrase monotonically
You also plot the dev set error

Your dev set error will go down and then begin to increase
You find the point where the dev set starts going up and stop the network at that point

1. Optimize cost function J, reduce bias 
2. Not overfit, reduce variation

It is already complicated to choose hyperparamters
You should think of nothing else than reducing J
Then think of nothing else than reducing variance
Use a completely different set of tools for 1 and 2

Thing of 1 and 2 independently, orthogonalization
If you just use L2 regularization then 1 and 2 will be doen together

# Normalizing Inputs
Different input features might be scaled differently
You subtract out the mean and then divide by the standard deviation

Use the same μ and σ^2 to normalize the training, dev, and test sets
If you use unnormalized values then you could get an elongated shape
This gives you very elongated contours
If you normalize then you are more likely to get a symmetric value
Normalized and rounded values allow you to work with a more direct path

If you input features come from different scales then you must normalize them

# Vanishing / Exploding Gradients
If g(z) = z
then 
y = W^[l] * W^[l - 1] * ... W^[2] * W^[1] * A^[0]

Lets say that each weight matrix is just a bit larger than 1 tmes the ID matrix
[ 1.5 0
 0 1.5] lets say that is W^[l]
 y_hat will be then be
 [1.5^l 0
 0 1.5^l]

 If you have a very deep neural network than the value of y can explode or vanish as each layer makes the values larger or smaller
 If all of your weights are above 1 or below -1 then you explode
 If your weights are between -1 and 1 then you don't vanish

 If you have tiny values then gradient descent will get very small values on the final sigmoid layer

# Weight Initialization for Deep Networks
Partial solution for deep network is a more careful choice of weights
The larger n is, the smaller you want w_i to be 
More nodes means you want the weights to be smaller

A reasonable choice would be to set the variance(w_i) to 1 / n where n is the number of features going into each neuron

Scale the values by sqrt{1 / inputs}
W^[l] - np.random.randn(shape) * np.sqrt(2 / n^[l - 1]) for ReLU
Use np.sqrt(1 / n^[l - 1]) for tanh: Xavier initialization
Use np.sqrt(2 / (n^[l - 1] + n^[l]))
This gives z a similar scale to 1

The number in the numerator is one of the hyperparameters you can tune

# Gradient checking
To do gradient checking
Reshape ALL of the parameters into a big vector called θ
Reshape ALL of the dParameters into a big vector called dθ
Is dθ the slope of J

J(θ) is our loss function
for each i in θ:
    dθ_approx[i] = (J(θ with the i element increased by ε) - J(θ with the i element decreased by ε)) / 2ε
    // We expect this to be close to dθ
// We can now compare dθ to dθ_approx
Find Euclidian distance between dθ and dθ_approx and normalize by the sum of the lengths of the vectors
ε = 10^-7 is reasonable
The value should be less than 10^-7

# Gradient Checking Implementation Notes
Don't use grad check when you train, only when you debug
Very long computation, only use it once
Doesn't work with dropout 
Remember to include the regularization term

