# Optimization Algorithms
26 April 2018

# Mini-batch Gradient Descent
Having fast optimization algorithm can speed stuff up
Finding a good model is very iterative

Vectorization allows us to efficiently compute on m exmaples
Processing the entire dataset by looking at the entire dataset is slow

Split up the training data into min-batches of size say 1000
You end up with a bunch of mini-batches X, Y pairs

Round brackets used for training examples
Square brackets used for layers
Curly brackets used for different mini-batches

X^{mb} has dimensions (n, mini-batch-size)
Y^{mb} has dimensions (1, mini-batch-size)

Batch is where you do the entire dataset at the same time
Now
Z^[1] = W^[1]X^{t} + b^[1]
A^[1] = g^[1](Z^[1])
...
A^[L] = g^[L](Z^[L])
Cost is the cost divided by the size of the mini-batch (not the entire dataset) 
and regularization is also divided by mini-batch size not m
We call this J^{t}

Forwardprob with X^{t}, Y^{t}
Compute J^{t}
Backprob to find dW^{t}, db^{t}

This is called one epoch of training, one pass through training set
With mini-batch, one epoch can be say 5000 iterations

# Understanding Mini-batch Gradient Descent
With BGD you always decrease (if it ever goes up then something is wrong)
With mBGD you might not decrease monotonically

## Choosing mini-batch size
mb size of m is just BGD
mb size of 1 is just Stochastic Gradient Descent (SGD)
SGD never converges while BGD converges

mb size should be between 1 and m
BGD is too slow
SGD loses the speedup from vectorization

Small training set (<2000): Use BGD 
Typical mb sizes are anywhere from 64 to 512
Make sure that each minibatch fits in the CPU/GPU memory

# Exponentially Weighted Averages
Each day is 0.9 * previous avverage + 0.1 * value

V_n = 0.9 V_(n-1) + 0.1 theta_n
You get an exponentially weighted average

V_t = ΒV_t-1 + (1-Β)θ_t
Arppiximately average over last 1 / (1 - β) days

# Understanding Exponentially Weighted Averages
Implementation

V = 0
V := βv + (1 - β)θ_t

# Bias Correction in Exponentially Weighted Averages
The exponential weight starts off much lower (no previous values to look at) because you start v_0 at 0

V_t / (1 - β^t) is a better predictor

# Gradient Descent with Momentum
On ellipses you might walk back and forth, oscilating back and forth
On iteration t compute dw, db on the current mini-batch

Compute V_dw which is an exponential weighted average of previous dw values
Compute V_db the same

V_dw = β*V_dw + (1 - β)*dW

W := W - αV_dW
b := b - αb_db

β becomes a new hyperparameter
β = 0.9 is a pretty strong value
In practice bias correction is not needed

# RMSprop
Root mean square prop
Another algorithm for improving descent

On iteration t:
    Compute dw, db on current mb
    S_dw = β * S_dw + (1 - β) dW^2  // Element wise squaring
    S_db = β * S_db + (1 - β) db^2
    W := W - α * dW / sqrt(S_dW)
    b := b - α * db / sqrt(S_db)

Scale dW and db by the square of the values
Dampens movement in directions with a lot of movement

Wouldn't this make growth slower in general?
You typically add ε to S_dw and S_db to ensure you don't get zeroes

# Adam Optimization Algorithm
Many optimization algorithm only work in a few cases
Momentum, RMS, and Adam have been shown to work well

Adam is RMS and Momentum

V_dW = 0, S_dW = 0, V_db = 0, S_db = 0
Iteration t:
    Compute dW, db using mb
    V_dW = β_1 * V_dW + (1 - β_1) * dW
    V_db = β_1 * V_db + (1 - β_1) * db
    S_dw = β_2 * S_dw + (1 - β_2) * dW^2
    S_db = β_2 * S_db + (1 - β_2) * db^2
    V_dW_corr = V_dW / (1 - β_1^t)
    V_db_corr = V_db / (1 - β_1^t)
    S_dW_corr = S_dW / (1 - β_2^t)
    S_db_corr = S_db / (1 - β_2^t)
    W := W - α * V_dW_corr / sqrt(S_dW_corr + ε)
    b := b - α * V_db_corr / sqrt(S_db_corr + ε)

α: learing rate (tune me)
β_1: momentum (recommended: 0.9) dW - first momemnt
β_2: root mean square (recommended: 0.999) dW^2 - second moment
ε: doesn't matter much (recommended 10^-8)

ADAM is adaptive moment estimation

# Learning Rate Decay
Slowly decreaes your learning rate over time
You want to wonder at the start downwards and converge towards the end
Epoch is pass through ALL training examples

Typical
α = α_0 / (1 + decay_rate * epoch_num) 

Exponential Decay
α = decay_rate ^ epoch_num * α_0

Root decay
α = k / sqrt{epoch_num} * α_0

Or discrete decay
Sometimes people do manually decay, manually watch it and manually change the values around a bit

Decay is one of the final things to change, below tuning α itself

# The Problem of Local Optima
In very high dimensions, most points with gradient 0 are not full local minima, they are saddle points
To be local minima all parameters need to bend up

Plateaus can be a problem
Derivatives close to 0 for a very long time
