# Deep Neural Networks
17 April 2018

## Deep L-layer Neural Network
Deep neural networks are netowkrs with more than 1 hidden layer
Logisitc regression is 0 hidden layers, 1 layer
Neural network is 1 hidden layer, 2 layers

Example: 4 layer NN
5, 5, 3 layers in each network
L is the number of layers
n^[L] denotes the number of nodes in layer l
n^[0] = 3 = n_x
n^[1] = 5
n^[2] = 5
n^[3] = 3
n^[4] = 1

a^[l] is the activations for layer L
a^[l] = g^[l](z^[l])
W^[l] and b^[l] used
y_hat = a^[L]

# Forward Propagation in a Deep Network
W^[1] is (5, 3)     b^[1] is (5, 1)
W^[2] is (5, 5)     b^[2] is (5, 1)
W^[3] is (3, 5)     b^[3] is (3, 1)
W^[4] is (1, 3)     b^[4] is (1, 1)

z^[1] = W^[1]x + b^[1]
a^[1] = g^[1](z^[1])

z^[2] = W^[2]a^[1] + b^[2]
a^[2] = g^[2](z^[2])

z^[i] = W^[i]a^[i - 1] + b^[i]
a^[i]= g^[i](z^[i])

This is all for a single example
X is an n x m matrix with n features and m training examples
ql
Z^[i] = W^[i]A^[i - 1] + b^[i]
A^[i] = =g^[i](Z^[i])
Different training examples are in different column vectors

You need to use a for loop to go through the different layers

# Getting Your Matrix Dimensions Right
Weight from size a to size b means (b, a) matrix or a matrix with b rows and a columns
Biases on a layer of size b means (b, 1) matrix (colmun vector)
Activation on a layer of size b means (b, m) matrix (column vector of examples)
Input comes in as an (n, m) matrix (column vectors of examples)

# Why Deep Representations?
Earlier layers will represent simple features while later layers will represent more complex features
Circuit theory:
There are functions that you may be able to compute with a "small" L-layer deep network that a shllow netowrk requires more hidden units to compute

# Building Blocks fo Deep Neural Netowrks
For layer L you have W^[l] and b^[l] 

Forward prop:
Input A^[l - 1], Output A^[l]
Z^[l] = W^[l]A^[l - 1] + b^[l]
A^[l] = g^[l](Z^[l])
Cache Z^[l]

Start backprop with derivative of -L

Backward prop:
Input dA^[l], Output dA^[l - 1]
Return dZ^[l], dW^[l], db^[l]

Update:
W^[l] = W^[l] - α * dW^[l]
b^[l] = b^[l] - α * db^[l]

# Forward and Backward Propagation
dZ^[l] = dA^[l] x g^[l]'(Z^[l])
dW^[l] = (1/m) * dZ^[l] * A^[l - 1]^T
db^[l] = (1/m) np.sum(dZ^[l], axis = 1, keepdims = True)
dA^[l - 1] = W^[l]^T * dZ^[l]

# Parameters vs Hyperparameters
Parameters are W and b
    
Hyperparameters are 
 - α (learning rate)
 - num_iterations
 - L (# hidden layers)
 - n^[l] (# hidden units)
 - Choice of activation function

You need to try out a bunch of different hyperparameters
Deep learning is a very empirical process

