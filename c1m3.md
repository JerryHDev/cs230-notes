# Shallow Neural Networks
14 April 2018

## Neural Networks Overview
In neural networks you stack together a bunch of signmoid / ReLU units / etc.
Each layer has a bunch of calculations on it 
features are X
params are W and B
z^[1] is the first hidden layer
z^[1] = W^[1] * x + b^[1]
a^[1] = σ(z^[1])
z^[2] = W^[2] * a^[1] + b^[2]
a^[2] = σ(z^[2])

Square brackets are for layers
Rounded brackets are for different training examples

You backprop through all of the layers

## Neural Network Representation
Input features is in the Input Layer
Hidden layers
Output is in the Output Layer

a^[0] can be the first layer
a stands for activation layer
a is the value of stuff after the activation function

Input + Hidden + Output is a 2 layer network
We never count the input layer

Each layer (except input) has W and b parameters associated with it
w^[j] is an m x n matrix if a^[j-1] is size m and a^[j] is size n
b^[j] is an n x 1 vector is a^[j] is size n

a^[n] made from z^[n] made from w^[n], b^[n], a^[n-1]

## Computating a Neural Network Output
a^[l]_i means node i in layer l
4x3 matrix means 4 layers in current layer and 3 in previous
4x1 vector would be for biases

z^[1] = W^[1] * X + b^[1]
Different nodes in a layer are stacked vertically
a^[1] = σ(z^[1])

x is the same as a^[0]

## Vectorizing Across Multiple Training Examples
a^[2](i) means 2nd layer and ith training example

Horizontally we do training examples
Vertical index is for different nodes of X

## Explanation for Vectorized Implementation
Matrix multiplication

## Activation Functions
In our neural network we used the sigmoid function
σ(z) = 1 / (1 + e^-z)
g(z) is our sigmoid function

tanh(z) is a good sigmoid function
tanh(z) goes from -1 to 1 and looks like sigmoid
tanh(z) = (e^z - e^(-z)) / (e^z + e^-z)

Using the tanh() is almost always better than the sigmoid because you get a mean of 0 
You often center the data before training with the tanh function

You usually use sigmoid() for your output layer because you want to be between 0 and 1
For other layers you can use the tanh() activation function

g^[1] means the activation function for the first non-input layer
Issue with tanh() is the gradient becomes very small
This makes it hard to train and slows down gradient descent

ReLU makes deep learning much much faster
ReLU(z) = max(0, z)
has derivative of 0 before 0
has derivative of z after z

Sigmoid is for output in a classification problem and nothing else
ReLU is the best for speedy training 
tanh() is the most accurate
Leaky ReLU has a slight constant positive slope below 0 so that gradient descent doesn't die
Leaky ReLU is often better than normal ReLU

## Why do you Need Non-Linear Activation Functions
Without non-linear activation functions you just get a standard linear regress
Why not use g(z) = z

If you just have linear activation functions then your output is just a linear combination of your features
You need nonlinear activation functions

Loss in linear regression (A - Y)^2
Loss in logistic regression Y * log(A) + (1 - Y) * log(1 - A)

Always consider what range of numbers can come out of each layer
You might use a linear activation function in the final layer to scale values for a regression model

## Derivatives of Activation Functions

### Sigmoid
g(z) = 1 / (1 + e^(-z))
g'(z) = g * (1 - g)

### Tanh
g(z) = (e^z - e^-z) / (e^z + e^-z)
g'(z) = 1 - (tanh(z))^2
g'(z) = 1 - g^2

### ReLU
g(z) = max(0, z)
g'(z) = 0 if g(z) < 0
g'(z) = 1 if g(z) > 0

## Leaky ReLU
g(z) = max(0.01, z)
g'(z) = 0.01 if g(z) < 0
g'(z) = 1 if g(z) > 0

## Gradient Descent for Neural Networks
With NN you need to initialize all your parameters to random numbers
in the backprop you just update each of the m with 

W^[i] = W^[i] - α * dw^[i]

Forward prop
Z^[1] = W^[1] * A^[0] + b^[1]       
A^[1] = g^[1](Z^[1])
Z^[2] = W^[2] * A^[1] + b^[2]
A^[2] = g^[2](Z^[2])

dz^[2] = A^[2] - Y
dw^[2] = (1/m) * dz^[2] * A^[1]^T
db^[2] = (1/m) * sum(dz^[2])
dz^[1] = W^[2]^T * dz^[2] x g^[1]' * Z^[1]
dw^[1] = (1/m) dz^[1] * X^T
db^[1] = (1/m) * sum(dz^[1])

## Random Initialization
You can't use all 0s on a NN, although you can for logistic regression
If its all 0s then each of the hiden units will have the same value
Each backprop will give the same update
All of your nodes in a layer will look the same

let's say you have 2 features and 2 hidden nodes

W^[1] = np.random.randn((2, 1)) * 0.01
b^[1] = np.zerp((2, 1))

Initialize W randomly and b to zeroes

