# cS 230
# Course 1 Module 2: Neural Networks Basics
# 9 April 2018

# Binary Classification
Logistic regression is a binary classification algorithm
To go from image to binary label
Make a single vector that lists all of the pixel values in the image
n is the size of the input vector
Learn a classifier that inputs an image that inputs a vector and outputs and cat or not

(x, y) is a training set
x is in R^n
y is in {0, 1}
m training examples: (x^(1), y^(1)) ... (x^(m), y^(m))
m is the number of training examples
n is the number of data points

Training matrix has each of our training vectors as a column
m columns and n rows
X is an n x m matrix

`X.shape()` gives the shape of the matrix

We stack the outputs into a column vector of height m
Y is a 1 x m matrix

# Logistic Regression
Used for binary classification
Want y_hat from x
y_hat is the P(y=1 | x)
Parameters are w (n column)

Output y_hat = w^T * x + b is a linear regression but doesn't give you stuff between 0 and 1
y_hat is a sigmoid(w^T*x + b)
Sigmoid maps from -inf, inf to 0, 1 wtih (0, 0.5)

# Logistic Regression Cost Function
y_hat = sigma(w^T * x + b)
σ(z) = 1 / (1 + e^(-z))

Loss function is L(y_hat, y) = 1/2 * (y_hat - y)^2
Loss(y_hat, y) = - (y log(y_hat) + (1 - y) * log(1 - y_hat))

Cost function is like loss except for the entire training set
J(w, b) is the average of the loss function 

# Gradient Descent
We want to find (w, b) that minimize J
J(w, b) is a convex function: we just happen to know this
Initialize w and b to an initial value
Gradient descent goes the steepest direction downwards
`w := w - α * J'(w, b) over w`
`b := b - α * J'(w, b) over b`

# Computation Graphs
