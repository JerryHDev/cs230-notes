# Hyperparamter Tuning
30 April 2018

# Tuning Process
There can be a lot of hyperparamters
Learning rates, layer count, layer size

Most important hyperparamters:
Alpha, Momentum, Mini-batch size, # of Hidden Units, # of Layer, Learning Rate Decay

How to figure out what values to 
It used to be common practice to explore values on a grid
That worked well with pre-NN machine learning

Choose the hyperparameters at random

Use a course to fine sampling scheme
Find a course set of points, then hone in on those points

# Using an Appropriate Scale to Pick Hyperparameters
ex: We are trying to choose # of hidden units for layer l
We think that values from 50 to 100 are good
ex: We are trying to choose L
We think that values should range from 2 to 4

Uniform / grid sampling may work
Uniform sampling doesn't work for all

Let's say that α = 0.0001 to α = 1 could work
We want to search on the log scale
This ensures you hit the range of values evenly
r = -4 * np.random.rand()
α = 10^r

When sampling β
We think that the range 0.9 to 0.999 are values
Sample between 0.001 to 0.1 with log scale
Sample from -3 to -1 and take r to the power of

# Pandas vs Caviar
Re-test your hyperparameters every now and then

Panda - Babysit one model
You change parameters manually over time

Caviar - Train many models in parallel
Run them all at once

# Normalizing Activations in a Network
Normalizing input features we know speeds up training as it makes it more uniformally concave
If you want to train say layer 3, could you normalize the activations of layer 2
This is what batch norm is 
This technically norms the Z values not the A values (done by default)

Given some intermediate values (Z^[i]):
    Calculate Z^[i]_norm (with added epsilon)
    Z^μ(i) = γZ^[i]_norm + β

    γ and β allows you to set the mean and variance back to normal
    γ = sqrt(σ^2 + ε)
    β = u

    Use Z^~[l](i)    
    Now your hidden values can have different values and means

# Fitting Batch Norm into a Neural Network
Without batch norm
X --> Z^[1] --> A^[1] --> Z^[2] --> A^[2] ...

With batch norm
X --> Z^[1] --> Z^~[1] --> A^[1] --> Z^[1] --> Z^~[2] --> A^[2]
batch norm goverened by β and γ

Batch norm makes the mean β and the std. dev. γ
Parameters: W^[1], b^[1], W^[2], b^[2] ...
Parameters: β^[1], γ^[1], β^[2], γ^[2] ...
β^[i] and γ^[i] are learnable parameters, not hyperparameters?

You still find dΒ^[l] and use learning methods like momentum, RMS, or ADAM to train

tf.nn.batch-normalization() can be done
You normally don't need to implement the details yourself

Batch norm is usually used with mini-batch
You take the first mini-batch and find Z^{1}
You take the mini-batch and adjust the values **only in the mini-batch** and rescale them into the β, γ range
You take the distribution across only a single mini-batch, not across all samples

Each layer has W, β, γ as parameters. No b.

Z^[l] = W^[l]a^[l-1] + b^[l]
During batch-norm you will end up subtracting out b^[l] during normalization
During batch-norm you don't use a bias

Z^[l] = W^[l]a^[l-1]
Z^![l] - γ^[l]Z^[l]_norm + β^[l]
We don't need the bias parameters

Z^[l] is (n^[l], m)
β^[l] is (n^[l], 1)
γ^[l] is (n^[l], 1)
Each hidden unit has its own β and γ

for t = 1 ... numMiniBatches
    Compute forward prop on X^{t}
        In each HL use BN to replace Z^[l] with Z^~[l]
    Use backprop to compute dW, dβ, dγ
    Update parameters with gradient descent / momentum / RMS prop / ADAM

# Why Does Batch Norm Work?
Normalizing input features is done within each feature across training examples not within each training example across features
This means you need a sufficiently large batch size in order to normalize

Batch norm makes weights in deeper layers less susceptible to movement in earlier layers
Covariate shifting is learning the X to Y mapping
If the distribution of X changes then you will need to change your mapping

Example:
Consider layer 3 of a 4 hidden layer network
This needs to learn W^[3] and b^[3] while knowing A^[2]
A^[2] might as well be features into a 2 hidden layer network
Layer 3 doesn't care where the values from layer 2 came from
The hidden unit values going into 3 change all the time because the weights in the previous layers keep changing

Batch norm reduces the amount by which this shifting affects future layers
Batch norm reduces the problem of the input changing
Stabilizes the values in future layers

Batch norm has a slight regularization effect
Each batch has its values scaled by the mean and variance of just the single mini-batch
The mean and variance have a bit of noise in them
The scaling process from Z^[l] to Z^~[l] is a bit noisy, this adds a bit of noise to each activation
This adds a bit of additive and multiplicative noise
Larger mini-batch size you reduce this regularization effect

Computes mean and variance on each mini-batch
This makes prediction on a single dataset a bit odd

# Batch Norm at Test Time
μ = mean 
σ^2 = mean of difference to mean^2
z^(i) =
z^~(i) = γz_norm + β

Each mini-batch gives you a different set of values of μ and σ
You could run the entire training set through the entire training set to get μ and σ
Alternatively you can get values of μ and σ through EMA in order to have the values for test time

# Softmax Regression
Binary classification gave us between 0 and 1
Softmax regression is to recognize different classes
Let's say you have 4 classes

C = # of classes
The final layer should have C units
y_hat is (C, 1)
The four numbers in y_hat should sum to 1

The final layer should be a softmax layer
Z^[L] = W^[L]a^[L-1] + b^[L]

t = e^(z^[L])
a^[L] = t / sum of t values

Just take the weighted ratios of the exponents of the different values
The algorithm maps the final vector into a probability density

a^[L] = g^[l](Z^[l])
The softmax inputs the enti re vector and outputs the entire vector

# Training a Softmax Classifier
If C = 2 then you just have logistic regression

Softmax loss function
ex:
Ground truth is y = [0, 1, 0, 0] as column
NN outputs y_hat = [0.3, 0.2, 0.1, 0.4] as column
L(y_hat, y) = - sum across C(y * log(y_hat))

Only looks at the true case and find the log of its probability which will always be negative
It is just the sum of negative log values of y_hat that matched
Use gradient descent to minimize the cost

Gradient descent with softmax
dZ^[l] = y_hat - y
**Question** How come dZ with softmax is just y_hat - y

# Deep Learning Framework
TensorFlow and friends
Caffee, CNTK, DL4J, Keras, Lasagne, mxnet, PaddlePaddle, Theano, Torch

# TensorFlow
ex: J(w) = w^2 - 10w  + 25 = (w - 5)^2

`w = tf.Variable(0, dtype=tf.float32)` # defines param
`cost = tf.add(tf.add(w**2,tf.multiply(-10,w)),25)`
`train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)`

`init = tf.gloabl_variables_initializer()`
`session = tf.Session()`
`session.run(init)`
`session.run(w)` # returns 0
`session.run(train)` # runs one step
`session.run(w)` # now returns 0.01

`for i in range(1000):`
`   session.run(train)`
`session.run(w)` # now returns almost exactly 5

The parameter we optimize should be defined as variables
You only need to do the forward prop and it figures out the backprop
`cost = w**2 - 10*w + 25` is already overloaded
This minimizes a fixed sample of w

Let's say we have training data x
`x = tf.placeholder(tf.float32, [3, 1])`
`cost = x[0][0] * w**2 + x[1][0] * w + x[2][0]`
`session.run(train, feed_dict={x:coefficients})` # is now what we use